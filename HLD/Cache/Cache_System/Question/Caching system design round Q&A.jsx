üîπ Caching System Design Round ‚Äì Q&A
1Ô∏è‚É£ Q: Why do we need caching in system design?

A:

To reduce database load

To decrease latency / response time

To handle high read traffic

To improve user experience

Example:

News feed in social media

Product catalog in e-commerce

Interview tip: Mention performance, scalability, and cost reduction.

2Ô∏è‚É£ Q: Where would you put a cache in an architecture?

A:

Between application servers and database (common)

At the edge (CDN) for static assets

In microservices for frequently accessed data

Example:

User ‚Üí Node.js API ‚Üí Redis Cache ‚Üí Database

3Ô∏è‚É£ Q: How do you choose what to cache?

A:

Read-heavy data

Data that doesn‚Äôt change often

Expensive DB queries / computation results

Avoid sensitive or write-heavy data

Example:

Product details, leaderboard, user profile info

4Ô∏è‚É£ Q: How do you handle cache invalidation?

A:

TTL (time-to-live) ‚Üí automatic expiry

Manual deletion on POST/PUT/DELETE

Pattern-based invalidation with SCAN

Event-based invalidation in distributed systems

Tip: Cache invalidation is one of the hardest parts. Always mention it.

5Ô∏è‚É£ Q: What caching patterns do you know?

A:

Cache-Aside (Lazy Loading) ‚Üí App controls cache

Read-Through ‚Üí Cache loads DB automatically

Write-Through ‚Üí Write to cache + DB together

Write-Behind ‚Üí Write to cache first, DB later

Session Store ‚Üí Cache stores session

Rate Limiting / Counters ‚Üí Cache stores counters

Mention examples in interviews, e.g., likes counter ‚Üí write-behind

6Ô∏è‚É£ Q: What are cache eviction policies?

A:

LRU (Least Recently Used) ‚Üí remove least used keys

LFU (Least Frequently Used) ‚Üí remove least accessed keys

TTL / Expiration ‚Üí remove keys after a set time

Random ‚Üí remove random keys when memory full

Example:

Redis default ‚Üí LRU approximation

7Ô∏è‚É£ Q: What happens if cache fails?

A:

Fallback to database

Application should continue working (degraded mode)

Don‚Äôt crash the system

Tip: Always mention fault-tolerance

8Ô∏è‚É£ Q: How do you avoid cache stampede?

A:

Locking / Mutex ‚Üí Only one process populates cache

Request coalescing / queuing

Randomized TTL ‚Üí avoid many keys expiring at the same time

Example:

Hot product page in e-commerce

9Ô∏è‚É£ Q: How do you scale Redis in production?

A:

Sharding ‚Üí Split keys across multiple Redis nodes

Replication ‚Üí Master ‚Üí replicas for read scaling

Cluster Mode ‚Üí For high availability and partitioning

Persistence (RDB / AOF) ‚Üí optional durability

üîü Q: When would you choose cache vs database query optimization?

A:

Cache ‚Üí If data is read-heavy, expensive to compute, or latency sensitive

Query optimization ‚Üí If write-heavy or DB can handle load efficiently

Combined ‚Üí Use indexing + cache for best results

üèóÔ∏è Bonus System Design Interview Tip

When asked:

‚ÄúDesign a caching layer for X system‚Äù

Answer structure:

Identify read-heavy / expensive data

Choose caching pattern (cache-aside, write-behind‚Ä¶)

Define TTL / eviction policy

Handle invalidation

Discuss scaling & failure handling

Example: ‚ÄúDesign cache for product catalog: Use Redis cache-aside, TTL 1 hour, LRU eviction, fallback to DB, horizontal Redis cluster for scaling.‚Äù

üß† Quick Memory Table (Interview Cheat Sheet)



| Question                  | Answer / Key Point                                     |
| ------------------------- | ------------------------------------------------------ |
| Why cache?                | Reduce DB load, lower latency, handle high traffic     |
| Where to put?             | Between API and DB, or edge (CDN)                      |
| What to cache?            | Read-heavy, non-sensitive, expensive queries           |
| Invalidation?             | TTL, manual delete, SCAN, event-based                  |
| Patterns?                 | Cache-aside, read-through, write-through, write-behind |
| Eviction?                 | LRU, LFU, TTL, Random                                  |
| Cache failure?            | Fallback to DB, graceful degradation                   |
| Cache stampede?           | Locking, request coalescing, random TTL                |
| Scaling?                  | Sharding, replication, clustering                      |
| Cache vs DB optimization? | Cache: read-heavy, expensive; DB: write-heavy          |



