Perfect! You already have a L1 + L2 cache + ETag setup. âœ…
Now letâ€™s add cache stampede prevention to your code so only one request fetches from DB when cache misses, while others wait or use stale data.

ğŸ”¹ Updated GetIo with Stampede Prevention
const Register = require("../model/student");
const redisClient = require("../Redis/redisClient");
const l1Cache = require("../cache"); // local cache

exports.GetIo = async (req, res) => {
  try {
    // 1ï¸âƒ£ Read version from Redis
    let version = await redisClient.get("students:version");
    if (!version) {
      version = "1";
      await redisClient.set("students:version", version);
    }

    const currentETag = `v${version}`;

    // ğŸ”¥ ETag check
    const clientETag = req.headers["if-none-match"];
    if (clientETag === currentETag) {
      return res.status(304).end();
    }

    const cacheKey = `students:list:v${version}:page:1`;
    const lockKey = `lock:${cacheKey}`;

    // 2ï¸âƒ£ Check L1 cache
    let cachedData = l1Cache.get(cacheKey);
    if (cachedData) {
      return res
        .set("ETag", currentETag)
        .status(200)
        .json({ source: "L1 cache", data: cachedData });
    }

    // 3ï¸âƒ£ Check L2 Redis
    cachedData = await redisClient.get(cacheKey);
    if (cachedData) {
      const parsedData = JSON.parse(cachedData);
      l1Cache.set(cacheKey, parsedData);
      return res
        .set("ETag", currentETag)
        .status(200)
        .json({ source: "Redis", data: parsedData });
    }

    // 4ï¸âƒ£ Acquire Redis lock to prevent cache stampede
    const lock = await redisClient.set(lockKey, "1", { NX: true, EX: 5 });
    if (!lock) {
      // Another request is rebuilding the cache
      // Option 1: Wait & retry (simple version: return "try again")
      return res.status(429).json({ message: "Cache is rebuilding, try again" });
    }

    // 5ï¸âƒ£ DB fallback
    const data = await Register.find();

    // 6ï¸âƒ£ Save to L2 and L1
    await redisClient.set(cacheKey, JSON.stringify(data), { EX: 60 });
    l1Cache.set(cacheKey, data);

    // 7ï¸âƒ£ Release lock
    await redisClient.del(lockKey);

    return res
      .set("ETag", currentETag)
      .status(200)
      .json({ source: "DB", data });

  } catch (error) {
    // Ensure lock is released in case of error
    await redisClient.del(`lock:students:list:v${version}:page:1`);
    console.error(error);
    res.status(500).send("Server Error");
  }
};

ğŸ”¹ How This Prevents Cache Stampede

Cache miss occurs in L1 + L2

Redis lock ensures only one request fetches DB

Other requests:

Wait or

Return 429 (or optionally return stale cache)

After DB fetch â†’ populate L2 + L1

Lock released â†’ next request reads fresh cache

ğŸ”¹ Optional Advanced Improvements

Return stale cache instead of 429:

Avoids error during high traffic

Add random TTL jitter:

Prevents all caches from expiring at the same time (avoids â€œcache avalancheâ€)

Promise-based wait queue:

Requests wait until lock is released, then read new cache

ğŸ”¹ Interview Explanation

â€œTo prevent cache stampede in GET APIs, I use a Redis distributed lock. When cache is missing, only one request fetches data from DB and updates both L2 (Redis) and L1 (local memory) caches. Other requests either wait or return stale data. This prevents database overload during high traffic.â€





ğŸ”¹ How This Prevents Cache Stampede

Cache miss occurs in L1 + L2

Redis lock ensures only one request fetches DB

Other requests:

Wait or

Return 429 (or optionally return stale cache)

After DB fetch â†’ populate L2 + L1

Lock released â†’ next request reads fresh cache

ğŸ”¹ Optional Advanced Improvements

Return stale cache instead of 429:

Avoids error during high traffic

Add random TTL jitter:

Prevents all caches from expiring at the same time (avoids â€œcache avalancheâ€)

Promise-based wait queue:

Requests wait until lock is released, then read new cache

ğŸ”¹ Interview Explanation

â€œTo prevent cache stampede in GET APIs, I use a Redis distributed lock. When cache is missing, only one request fetches data from DB and updates both L2 (Redis) and L1 (local memory) caches. Other requests either wait or return stale data. This prevents database overload during high traffic.â€